# Projeto: Sistema de Troca de Mensagens InstantÃ¢neas

Sistema distribuÃ­do de mensagens inspirado em BBS/IRC, implementado com ZeroMQ e MessagePack.

> ğŸ“‘ **NavegaÃ§Ã£o**: Veja [INDEX.md](INDEX.md) para Ã­ndice completo da documentaÃ§Ã£o
> 
> âš¡ **Quick Start**: Veja [QUICKSTART.md](QUICKSTART.md) para comeÃ§ar em 5 minutos

## DescriÃ§Ã£o

Este projeto implementa um sistema completo de mensagens instantÃ¢neas com suporte a:
- Login de usuÃ¡rios
- Canais pÃºblicos para discussÃµes
- Mensagens privadas entre usuÃ¡rios
- PersistÃªncia de dados
- ReplicaÃ§Ã£o de dados entre servidores
- RelÃ³gios lÃ³gicos para ordenaÃ§Ã£o de eventos
- SincronizaÃ§Ã£o de relÃ³gio fÃ­sico (Algoritmo de Berkeley)

## Arquitetura

O sistema Ã© composto por 6 tipos de containers:

1. **Broker** (Python) - Balanceamento de carga Request-Reply (ROUTER-DEALER)
2. **Proxy** (Python) - Proxy Pub/Sub (XSUB-XPUB)
3. **Reference** (Python) - Servidor de referÃªncia para gerenciamento de servidores
4. **Server** (Python) - Servidores que processam requisiÃ§Ãµes (3 rÃ©plicas)
5. **Client** (Node.js) - Interface interativa para usuÃ¡rios
6. **Bot** (Go) - Bot automÃ¡tico que gera mensagens aleatÃ³rias (2 rÃ©plicas)

### Diagrama de ConexÃµes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reference  â”‚â—„â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                      â”‚ REQ/REP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  Broker  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚      â”‚
     â”‚ REQ/REP        â”‚      â”‚
     â”‚                â”‚      â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”   â”‚
â”‚ Server 1 â”‚Serverâ”‚Serverâ”‚   â”‚
â”‚          â”‚  2   â”‚  3   â”‚   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”˜   â”‚
      â”‚               â”‚      â”‚
      â”‚ PUB           â”‚      â”‚
      â–¼               â–¼      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  Proxy  â”‚                  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â”‚
     â”‚ SUB                   â”‚
     â–¼                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ Client â”‚ Bot 1â”‚ Bot 2â”‚â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

## Linguagens Utilizadas

Conforme requisito do projeto, foram utilizadas 3 linguagens diferentes:

1. **Python** - Broker, Proxy, Servidor de ReferÃªncia e Servidores principais
2. **JavaScript (Node.js)** - Cliente interativo
3. **Go** - Bot automÃ¡tico

## Funcionalidades Implementadas

### Parte 1: Request-Reply
- âœ… Login de usuÃ¡rios
- âœ… Listagem de usuÃ¡rios cadastrados
- âœ… CriaÃ§Ã£o de canais
- âœ… Listagem de canais
- âœ… PersistÃªncia de dados em disco

### Parte 2: Publisher-Subscriber
- âœ… PublicaÃ§Ã£o em canais pÃºblicos
- âœ… Mensagens privadas entre usuÃ¡rios
- âœ… Bot automÃ¡tico que gera mensagens
- âœ… PersistÃªncia de mensagens e publicaÃ§Ãµes

### Parte 3: MessagePack
- âœ… ConversÃ£o de JSON para MessagePack
- âœ… SerializaÃ§Ã£o binÃ¡ria em todas as comunicaÃ§Ãµes
- âœ… Compatibilidade entre as 3 linguagens

### Parte 4: RelÃ³gios
- âœ… RelÃ³gio lÃ³gico em todos os processos
- âœ… Servidor de referÃªncia para gerenciamento
- âœ… Sistema de rank para servidores
- âœ… Heartbeat periÃ³dico
- âœ… Base para sincronizaÃ§Ã£o de Berkeley (estrutura implementada)
- âœ… Base para eleiÃ§Ã£o de coordenador (estrutura implementada)

### Parte 5: ConsistÃªncia e ReplicaÃ§Ã£o
- âœ… ReplicaÃ§Ã£o de todos os dados entre servidores
- âœ… SincronizaÃ§Ã£o automÃ¡tica via Pub/Sub

## ImplementaÃ§Ã£o da ReplicaÃ§Ã£o (Parte 5)

### MÃ©todo Escolhido: ReplicaÃ§Ã£o PrimÃ¡ria com Broadcast

O sistema implementa um modelo de **replicaÃ§Ã£o ativa** onde:

1. **Todos os servidores sÃ£o iguais** - Qualquer servidor pode processar qualquer requisiÃ§Ã£o
2. **ReplicaÃ§Ã£o via Pub/Sub** - Quando um servidor processa uma operaÃ§Ã£o que altera o estado (login, criaÃ§Ã£o de canal, mensagem, publicaÃ§Ã£o), ele:
   - Salva os dados localmente
   - Publica a operaÃ§Ã£o no tÃ³pico `replication`
   - Outros servidores recebem e aplicam a mesma operaÃ§Ã£o

3. **EvitaÃ§Ã£o de Loops** - Cada mensagem de replicaÃ§Ã£o contÃ©m o ID do servidor de origem, evitando que o servidor reaplique suas prÃ³prias operaÃ§Ãµes

4. **RelÃ³gio LÃ³gico para OrdenaÃ§Ã£o** - Todas as operaÃ§Ãµes incluem o relÃ³gio lÃ³gico, permitindo ordenaÃ§Ã£o consistente de eventos

### OperaÃ§Ãµes Replicadas

As seguintes operaÃ§Ãµes sÃ£o automaticamente replicadas:

- `login` - Cadastro de novos usuÃ¡rios
- `channel` - CriaÃ§Ã£o de canais
- `message` - Mensagens privadas entre usuÃ¡rios
- `publication` - PublicaÃ§Ãµes em canais

### Vantagens da Abordagem

1. **Simplicidade** - Usa a infraestrutura Pub/Sub jÃ¡ existente
2. **Escalabilidade** - Novos servidores automaticamente recebem atualizaÃ§Ãµes
3. **TolerÃ¢ncia a Falhas** - Se um servidor cai, os outros mantÃªm os dados
4. **ConsistÃªncia Eventual** - Todos os servidores convergem para o mesmo estado

### LimitaÃ§Ãµes

1. **NÃ£o hÃ¡ consenso forte** - Em caso de operaÃ§Ãµes conflitantes, a ordem depende do relÃ³gio lÃ³gico
2. **Sem recuperaÃ§Ã£o de histÃ³rico** - Servidores novos nÃ£o recebem dados anteriores (poderia ser implementado)
3. **Broadcast overhead** - Todas as operaÃ§Ãµes sÃ£o enviadas para todos os servidores

## Como Executar

### PrÃ©-requisitos

- Docker
- Docker Compose

### Iniciar o Sistema

```bash
cd src
docker compose up --build
```

O sistema iniciarÃ¡ automaticamente:
- 1 Broker (porta 5555-5556)
- 1 Proxy (porta 5557-5558)
- 1 Servidor de ReferÃªncia (porta 5559)
- 3 Servidores (replicados)
- 1 Cliente interativo
- 2 Bots automÃ¡ticos

### Parar o Sistema

```bash
cd src
docker compose down
```

### Limpar Dados e Reconstruir

```bash
cd src
docker compose down --rmi all --volumes
docker compose up --build
```

## Estrutura de Dados

### Formato das Mensagens (MessagePack)

Todas as mensagens seguem o formato:

```json
{
  "service": "nome_do_serviÃ§o",
  "data": {
    "clock": 123,
    "timestamp": "2025-11-12T...",
    ...
  }
}
```

### PersistÃªncia

Os dados sÃ£o salvos em arquivos JSON no volume `server-data`:

- `users.json` - UsuÃ¡rios e seus timestamps de login
- `channels.json` - Lista de canais criados
- `messages.json` - HistÃ³rico de mensagens privadas
- `publications.json` - HistÃ³rico de publicaÃ§Ãµes em canais

## ReferÃªncia RÃ¡pida de Comandos

### Comandos Essenciais

```bash
# Iniciar sistema
cd src && docker compose up --build

# Parar sistema
docker compose down

# Conectar ao cliente interativo
docker exec -it src-client-1 node main.js

# Ver logs de todos os serviÃ§os
docker compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker compose logs -f server
docker compose logs -f bot
docker compose logs -f broker

# Ver status dos containers
docker compose ps

# Parar um container especÃ­fico
docker stop src-server-1

# Reiniciar um container
docker restart src-server-1

# Ver dados persistidos
docker exec src-server-1 cat /app/data/users.json
```

## Como Testar as Funcionalidades

### 1. Testar Login de UsuÃ¡rios

```bash
# Conectar ao cliente interativo
docker exec -it src-client-1 node main.js
```

No menu do cliente:
1. Digite `1` para fazer login
2. Digite seu nome de usuÃ¡rio
3. Verifique nos logs que todos os servidores receberam a replicaÃ§Ã£o:
   ```bash
   docker compose logs server | grep "Replicando: login"
   ```

### 2. Testar CriaÃ§Ã£o de Canais

No cliente:
1. Digite `3` para criar canal
2. Digite o nome do canal (ex: `geral`)
3. Verifique a replicaÃ§Ã£o nos logs

### 3. Testar PublicaÃ§Ã£o em Canais

No cliente:
1. Digite `4` para listar canais
2. Digite `5` para publicar em um canal
3. Digite o nome do canal e a mensagem

Para ver as mensagens sendo recebidas:
```bash
# Em outro terminal, conecte outro cliente
docker exec -it src-client-1 node main.js
```

### 4. Testar Mensagens Privadas

1. Liste os usuÃ¡rios conectados (opÃ§Ã£o `2`)
2. Envie uma mensagem privada (opÃ§Ã£o `6`)
3. Digite o nome do destinatÃ¡rio e a mensagem

### 5. Testar Bots AutomÃ¡ticos

Os bots automaticamente:
- Fazem login com nome aleatÃ³rio (`bot_XXXXX`)
- Buscam canais disponÃ­veis
- Publicam 10 mensagens em canais aleatÃ³rios
- Aguardam e repetem o processo

Ver atividade dos bots:
```bash
docker compose logs -f bot
```

### 6. Testar ReplicaÃ§Ã£o de Dados

**Teste de ConsistÃªncia:**

1. Crie um canal usando o cliente
2. Pare um dos servidores:
   ```bash
   docker stop src-server-1
   ```
3. Crie outro canal
4. Reinicie o servidor parado:
   ```bash
   docker start src-server-1
   ```
5. O servidor voltarÃ¡ com os dados antigos, mas pode nÃ£o ter o canal criado durante sua ausÃªncia
6. Liste os canais - vocÃª verÃ¡ diferentes resultados dependendo de qual servidor atende

**Ver logs de replicaÃ§Ã£o:**
```bash
docker compose logs server | grep "Replicando\|Recebendo replicaÃ§Ã£o"
```

### 7. Testar RelÃ³gios LÃ³gicos

Observe nos logs que cada operaÃ§Ã£o incrementa o relÃ³gio lÃ³gico:
```bash
docker compose logs server | grep "Clock="
```

Exemplo de saÃ­da:
```
server-1 | [server_5067 Clock=10] Recebido: login
server-2 | [server_1956 Clock=15] Recebido: channels
server-3 | [server_3121 Clock=18] Recebido: publish
```

### 8. Testar Sistema de Ranks

Quando os servidores iniciam, eles recebem um rank do servidor de referÃªncia:
```bash
docker compose logs reference
```

SaÃ­da esperada:
```
reference | [Clock=2] Recebido: rank
reference | [Clock=6] Respondido: rank
server-1  | [server_5067] Rank recebido: 0, Clock: 3
server-2  | [server_1956] Rank recebido: 1, Clock: 7
server-3  | [server_3121] Rank recebido: 2, Clock: 11
```

### 9. Testar Heartbeat

Os servidores enviam heartbeat a cada 10 segundos:
```bash
docker compose logs reference | grep "heartbeat"
```

### 10. Verificar PersistÃªncia de Dados

Os dados sÃ£o salvos em um volume Docker:
```bash
# Ver arquivos persistidos
docker exec src-server-1 ls -la /app/data/

# Ver conteÃºdo dos arquivos
docker exec src-server-1 cat /app/data/users.json
docker exec src-server-1 cat /app/data/channels.json
docker exec src-server-1 cat /app/data/messages.json
docker exec src-server-1 cat /app/data/publications.json
```

## Simulando Falhas e EleiÃ§Ã£o de Coordenador

### CenÃ¡rio 1: Falha de um Servidor (NÃ£o-Coordenador)

```bash
# 1. Ver qual servidor tem o menor rank (serÃ¡ o coordenador)
docker compose logs server | grep "Rank recebido"

# 2. Parar um servidor que NÃƒO seja o de rank 0
docker stop src-server-2

# 3. Sistema continua funcionando normalmente com os outros 2 servidores
docker compose logs -f server

# 4. Reiniciar o servidor
docker start src-server-2

# 5. Servidor se reintegra automaticamente
```

### CenÃ¡rio 2: Falha do Coordenador

```bash
# 1. Identificar o coordenador (servidor com menor rank, geralmente rank=0)
docker compose logs server | grep "Rank recebido"

# 2. Parar o servidor coordenador
# Se o server-1 tiver rank 0:
docker stop src-server-1

# 3. Observar nos logs dos servidores restantes
docker compose logs -f server

# 4. Os servidores detectam a falta de heartbeat do coordenador
# (O sistema estÃ¡ preparado para eleiÃ§Ã£o, mas a implementaÃ§Ã£o 
# completa do Bully Algorithm estÃ¡ na estrutura base)

# 5. Para forÃ§ar uma nova eleiÃ§Ã£o manualmente:
# Reinicie o servidor de referÃªncia
docker restart reference

# 6. Todos os servidores se re-registram e recebem novos ranks
```

### CenÃ¡rio 3: Falha do Broker (Ponto Ãšnico de Falha)

```bash
# 1. Parar o broker
docker stop broker

# 2. Clientes nÃ£o conseguem mais fazer requisiÃ§Ãµes
# (Mensagens via Pub/Sub continuam funcionando)

# 3. Reiniciar broker
docker start broker

# 4. Sistema volta ao normal
```

### CenÃ¡rio 4: Falha do Proxy (Pub/Sub)

```bash
# 1. Parar o proxy
docker stop proxy

# 2. RequisiÃ§Ãµes Request/Reply continuam funcionando
# 3. Mas publicaÃ§Ãµes em canais e mensagens privadas param

# 4. Reiniciar proxy
docker start proxy

# 5. Pub/Sub volta a funcionar
```

### CenÃ¡rio 5: PartiÃ§Ã£o de Rede

```bash
# Simular partiÃ§Ã£o isolando um servidor
docker network disconnect src_messaging src-server-1

# Servidor fica isolado, nÃ£o recebe replicaÃ§Ãµes
# Observar comportamento nos logs
docker compose logs -f server

# Reconectar servidor
docker network connect src_messaging src-server-1
```

### CenÃ¡rio 6: Falha MÃºltipla

```bash
# Parar 2 dos 3 servidores
docker stop src-server-1 src-server-2

# Sistema continua com apenas 1 servidor
docker compose logs -f server

# Reiniciar servidores
docker start src-server-1 src-server-2
```

### Monitoramento Durante Falhas

**Terminal 1 - Logs gerais:**
```bash
docker compose logs -f
```

**Terminal 2 - Logs do servidor de referÃªncia:**
```bash
docker compose logs -f reference
```

**Terminal 3 - Status dos containers:**
```bash
watch -n 2 'docker compose ps'
```

**Terminal 4 - Cliente interativo:**
```bash
docker exec -it src-client-1 node main.js
```

### ObservaÃ§Ãµes sobre EleiÃ§Ã£o de Coordenador

O sistema possui a **infraestrutura base** para eleiÃ§Ã£o de coordenador:

1. **Sistema de Ranks**: Cada servidor tem um rank Ãºnico atribuÃ­do pelo servidor de referÃªncia
2. **Heartbeats**: Servidores enviam heartbeat periÃ³dico para detecÃ§Ã£o de falhas
3. **DetecÃ§Ã£o de Falhas**: Servidor de referÃªncia remove servidores sem heartbeat apÃ³s 30 segundos
4. **Canal de NotificaÃ§Ã£o**: Existe um tÃ³pico `servers` para notificar sobre mudanÃ§as no coordenador

**Para implementaÃ§Ã£o completa do Bully Algorithm:**

1. Quando um servidor detecta falta do coordenador (via heartbeat)
2. Ele inicia uma eleiÃ§Ã£o enviando mensagem ELECTION para servidores com rank maior
3. Se receber OK de algum servidor, aguarda mensagem COORDINATOR
4. Se nÃ£o receber resposta, se declara coordenador e envia COORDINATOR para todos
5. Servidor de referÃªncia valida e propaga a informaÃ§Ã£o via tÃ³pico `servers`

A estrutura estÃ¡ pronta em `server/main.py` (linhas 507-518):
```python
if topic_str == "servers":
    self.coordinator = data.get("coordinator")
    print(f"[{self.server_name}] Novo coordenador: {self.coordinator}")
```

### Ver Estado do Sistema em Tempo Real

```bash
# Ver todos os containers rodando
docker compose ps

# Ver uso de recursos
docker stats

# Ver logs de todos os serviÃ§os
docker compose logs --tail=50 -f

# Ver apenas mensagens importantes
docker compose logs | grep -E "Clock=|Rank|Replicando|coordenador"
```

## ResoluÃ§Ã£o de Problemas

### Bot nÃ£o consegue fazer login

**Erro:** `Erro no login: erro no login: ServiÃ§o desconhecido: None`

**SoluÃ§Ã£o:** Verifique se os servidores estÃ£o rodando:
```bash
docker compose ps
docker compose logs server
```

Se os servidores nÃ£o mostrarem logs de inicializaÃ§Ã£o, reconstrua as imagens:
```bash
docker compose down --rmi all
docker compose up --build
```

### Python nÃ£o mostra logs

**Problema:** ServiÃ§os Python nÃ£o exibem saÃ­da no console

**SoluÃ§Ã£o:** JÃ¡ corrigido! O Dockerfile.python usa `python -u` para saÃ­da nÃ£o-bufferizada

### Erro de build do Go

**Erro:** `missing go.sum entry for module providing package`

**SoluÃ§Ã£o:** JÃ¡ corrigido! O Dockerfile.go copia `main.go` antes de executar `go mod tidy`

### Cliente nÃ£o conecta

**Problema:** Cliente fica travado ou nÃ£o responde

**SoluÃ§Ã£o:**
```bash
# Reinicie o cliente
docker compose restart client

# Ou conecte manualmente
docker exec -it src-client-1 node main.js
```

### Portas em uso

**Erro:** `Bind for 0.0.0.0:5555 failed: port is already allocated`

**SoluÃ§Ã£o:**
```bash
# Ver o que estÃ¡ usando a porta
sudo lsof -i :5555

# Parar todos os containers
docker compose down

# Se necessÃ¡rio, matar processos na porta
sudo kill -9 <PID>
```

### Limpar tudo e comeÃ§ar do zero

```bash
# Remove containers, imagens, volumes e redes
cd src
docker compose down --rmi all --volumes --remove-orphans

# Remove imagens Ã³rfÃ£s
docker image prune -a

# ReconstrÃ³i tudo
docker compose up --build
```

### Ver erros detalhados

```bash
# Logs com timestamps
docker compose logs --timestamps

# Logs de um container especÃ­fico
docker logs src-server-1 --tail=100

# Seguir logs em tempo real
docker compose logs -f --tail=100
```

## Melhorias Futuras

- [ ] Implementar sincronizaÃ§Ã£o completa do Algoritmo de Berkeley
- [ ] Implementar eleiÃ§Ã£o de coordenador completa (Bully Algorithm)
- [ ] Adicionar recuperaÃ§Ã£o de histÃ³rico para servidores novos
- [ ] Implementar compactaÃ§Ã£o de logs
- [ ] Adicionar autenticaÃ§Ã£o com senhas
- [ ] Interface web para o cliente
- [ ] MÃ©tricas e monitoramento
- [ ] Testes automatizados
- [ ] CI/CD pipeline

## Autor

Desenvolvido como projeto da disciplina de Sistemas DistribuÃ­dos.

## LicenÃ§a

Este projeto Ã© para fins educacionais.
